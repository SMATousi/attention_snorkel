{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9eccfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48801d8d",
   "metadata": {},
   "source": [
    "# Creating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acbff5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "X, y = make_classification(n_samples=20000, n_features=3, n_informative=3, n_redundant=0,\n",
    "                           n_clusters_per_class=1, weights=[0.5, 0.5], flip_y=0.05, class_sep=1.5)\n",
    "y = 2*y - 1\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 6))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "\n",
    "# ax.scatter(X[y == -1][:, 0], X[y == -1][:, 1], X[y == -1][:, 2], c='b', marker='o', label='Class -1')\n",
    "# ax.scatter(X[y == 1][:, 0], X[y == 1][:, 1], X[y == 1][:, 2], c='r', marker='^', label='Class 1')\n",
    "\n",
    "# ax.set_xlabel('Feature 1')\n",
    "# ax.set_ylabel('Feature 2')\n",
    "# ax.set_zlabel('Feature 3')\n",
    "# ax.set_title('3D Scatter Plot of Synthetic Data')\n",
    "# ax.legend()\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b560b2b",
   "metadata": {},
   "source": [
    "# LFs generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a739d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_label_flip_and_zero(arr, m, n_list, zero_n_list):\n",
    "\n",
    "    if len(n_list) != m or len(zero_n_list) != m:\n",
    "        raise ValueError(\"The length of n_list and zero_n_list must be equal to m.\")\n",
    "    \n",
    "    length = len(arr)\n",
    "    flipped_arrays = []\n",
    "\n",
    "    for i in range(m):\n",
    "        n = n_list[i]\n",
    "        zero_n = zero_n_list[i]\n",
    "\n",
    "        # Randomly select indices to flip\n",
    "        indices_to_zero = np.random.choice(length, zero_n, replace=False)\n",
    "\n",
    "        # Create a copy of the array to flip the labels\n",
    "        modified_arr = arr.copy()\n",
    "        modified_arr[indices_to_zero] = 0\n",
    "\n",
    "        # Identify the untouched indices\n",
    "        untouched_indices = np.setdiff1d(np.arange(length), indices_to_zero)\n",
    "\n",
    "        # Randomly select indices from the untouched indices to set to 0\n",
    "        indices_to_flip = np.random.choice(untouched_indices, n, replace=False)\n",
    "\n",
    "        # Set the chosen indices to 0\n",
    "        modified_arr[indices_to_flip] = -modified_arr[indices_to_flip]\n",
    "\n",
    "        flipped_arrays.append(modified_arr)\n",
    "\n",
    "    return flipped_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f71dae1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1329, 1404, 1344, 1288, 999]\n"
     ]
    }
   ],
   "source": [
    "arr = y\n",
    "\n",
    "m = 5  \n",
    "\n",
    "beta_list = [0.35, 0.39, 0.42, 0.46, 0.5]\n",
    "zero_n_list = [int((1 - beta)* y.shape[0]) for beta in beta_list]  \n",
    "\n",
    "alpha_list = [0.81, 0.82, 0.84, 0.86, 0.90]\n",
    "n_list = [int((1-alpha)*(y.shape[0] - zero_n_list[i])) for i, alpha in enumerate(alpha_list)] \n",
    "\n",
    "print(n_list)\n",
    "\n",
    "flipped_arrays = random_label_flip_and_zero(arr, m, n_list, zero_n_list)\n",
    "\n",
    "\n",
    "# print(\"Original = \", arr)\n",
    "ALL_LFs = {}\n",
    "\n",
    "for i, modified_arr in enumerate(flipped_arrays):\n",
    "#     print(f\"Array {i+1}:\")\n",
    "#     print(modified_arr-arr)\n",
    "    lf_dict = {}\n",
    "    \n",
    "    lf_dict['alpha'] = 1 - (n_list[i]/(len(y) - zero_n_list[i]))\n",
    "    lf_dict['beta'] = 1 - (zero_n_list[i]/len(y))\n",
    "    \n",
    "    lf_dict['outputs'] = modified_arr\n",
    "    \n",
    "    ALL_LFs[i] = lf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df67f122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'alpha': 0.8101428571428572,\n",
       "  'beta': 0.35,\n",
       "  'outputs': array([-1,  0,  0, ...,  0,  1,  0])},\n",
       " 1: {'alpha': 0.8200000000000001,\n",
       "  'beta': 0.39,\n",
       "  'outputs': array([0, 1, 0, ..., 0, 0, 0])},\n",
       " 2: {'alpha': 0.84,\n",
       "  'beta': 0.42000000000000004,\n",
       "  'outputs': array([-1, -1,  0, ...,  0,  0,  1])},\n",
       " 3: {'alpha': 0.86,\n",
       "  'beta': 0.45999999999999996,\n",
       "  'outputs': array([-1,  0,  1, ...,  0,  1,  1])},\n",
       " 4: {'alpha': 0.9001,\n",
       "  'beta': 0.5,\n",
       "  'outputs': array([-1, -1,  1, ..., -1,  0,  1])}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_LFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1dcd23",
   "metadata": {},
   "source": [
    "# Expected Value for alpha and beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ea19610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum cardinality =  18870.345472180805\n",
      "current cardinality =  20000\n",
      "Check!\n"
     ]
    }
   ],
   "source": [
    "m = 5\n",
    "epsilon = 0.2\n",
    "s_cardinality = len(y)\n",
    "\n",
    "minimum_cardinality = (356/(epsilon)**2) * np.log(m/(3*epsilon))\n",
    "\n",
    "print(\"minimum cardinality = \", minimum_cardinality)\n",
    "print(\"current cardinality = \", s_cardinality)\n",
    "if s_cardinality > minimum_cardinality:\n",
    "    print(\"Check!\")\n",
    "else:\n",
    "    print(\"More data needed ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7495d1",
   "metadata": {},
   "source": [
    "# Label Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65643654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing\n",
    "\n",
    "Alpha_Beta_numpy = np.random.rand(m,2)\n",
    "Alpha_Beta = torch.tensor(Alpha_Beta_numpy, requires_grad=True)\n",
    "\n",
    "class LabelModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LabelModel, self).__init__()\n",
    "#         self.sigmoid = torch.sigmoid()\n",
    "        Alpha_Beta_numpy = np.random.rand(m,2)\n",
    "        self.alpha_beta_array = nn.Parameter(torch.tensor(Alpha_Beta_numpy, requires_grad=True))\n",
    "        \n",
    "    def forward(self, lf_label, true_label):\n",
    "        \n",
    "        all_lf_probls = 1\n",
    "        \n",
    "        for lf_index in range(self.alpha_beta_array.shape[0]):\n",
    "            \n",
    "            lf_alpha = torch.sigmoid(self.alpha_beta_array[lf_index,0])\n",
    "            lf_beta = torch.sigmoid(self.alpha_beta_array[lf_index,1])\n",
    "            \n",
    "            if lf_label[lf_index] == true_label:\n",
    "                \n",
    "                lf_prob = lf_alpha * lf_beta\n",
    "            \n",
    "            if lf_label[lf_index] == -true_label:\n",
    "                \n",
    "                lf_prob = (1 - lf_alpha) * lf_beta\n",
    "            \n",
    "            if lf_label[lf_index] == 0:\n",
    "                \n",
    "                lf_prob = 1 - lf_beta\n",
    "        \n",
    "            all_lf_probls = all_lf_probls * lf_prob\n",
    "        \n",
    "        \n",
    "        return 0.5 * all_lf_probls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed7a13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LabelModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2502954",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(alpha_beta_array=test_lf,\n",
    "      lf_label=test_lf_label,\n",
    "      true_label=test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e4ae5c",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d886c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LF_Output_Dataset(Dataset):\n",
    "    def __init__(self, ALL_LFs, X):\n",
    "        \n",
    "        self.ALL_LFs = ALL_LFs\n",
    "        self.X = X\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        data_sample = self.X[idx]\n",
    "        lf_outputs = []\n",
    "        \n",
    "        for key in self.ALL_LFs.keys():\n",
    "            \n",
    "            lf_outputs.append(self.ALL_LFs[key]['outputs'][idx])\n",
    "        \n",
    "        return data_sample, lf_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd3ad32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LF_Output_Dataset(ALL_LFs, X)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4052efa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f51e23a5",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a572699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 4.991134442566564\n",
      "tensor([[0.6290, 0.6739],\n",
      "        [0.5246, 0.5895],\n",
      "        [0.5525, 0.6610],\n",
      "        [0.6704, 0.7310],\n",
      "        [0.6128, 0.6204]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 1: Loss = 4.991029273344807\n",
      "tensor([[0.6290, 0.6739],\n",
      "        [0.5246, 0.5894],\n",
      "        [0.5525, 0.6610],\n",
      "        [0.6704, 0.7311],\n",
      "        [0.6129, 0.6204]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 2: Loss = 4.990924106683717\n",
      "tensor([[0.6290, 0.6739],\n",
      "        [0.5246, 0.5894],\n",
      "        [0.5525, 0.6610],\n",
      "        [0.6704, 0.7311],\n",
      "        [0.6129, 0.6204]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 3: Loss = 4.990818942583238\n",
      "tensor([[0.6290, 0.6739],\n",
      "        [0.5246, 0.5894],\n",
      "        [0.5526, 0.6610],\n",
      "        [0.6704, 0.7311],\n",
      "        [0.6129, 0.6204]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 4: Loss = 4.990713781043323\n",
      "tensor([[0.6290, 0.6739],\n",
      "        [0.5246, 0.5894],\n",
      "        [0.5526, 0.6610],\n",
      "        [0.6704, 0.7311],\n",
      "        [0.6129, 0.6204]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 5: Loss = 4.9906086220639185\n",
      "tensor([[0.6290, 0.6739],\n",
      "        [0.5246, 0.5894],\n",
      "        [0.5526, 0.6610],\n",
      "        [0.6704, 0.7311],\n",
      "        [0.6129, 0.6204]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 6: Loss = 4.9905034656449745\n",
      "tensor([[0.6290, 0.6739],\n",
      "        [0.5246, 0.5894],\n",
      "        [0.5526, 0.6610],\n",
      "        [0.6704, 0.7311],\n",
      "        [0.6129, 0.6204]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 7: Loss = 4.990398311786439\n",
      "tensor([[0.6290, 0.6739],\n",
      "        [0.5246, 0.5894],\n",
      "        [0.5526, 0.6610],\n",
      "        [0.6704, 0.7311],\n",
      "        [0.6129, 0.6204]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 8: Loss = 4.990293160488262\n",
      "tensor([[0.6290, 0.6739],\n",
      "        [0.5246, 0.5893],\n",
      "        [0.5526, 0.6610],\n",
      "        [0.6704, 0.7311],\n",
      "        [0.6129, 0.6205]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 9: Loss = 4.990188011750392\n",
      "tensor([[0.6290, 0.6739],\n",
      "        [0.5246, 0.5893],\n",
      "        [0.5526, 0.6610],\n",
      "        [0.6704, 0.7311],\n",
      "        [0.6129, 0.6205]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 10: Loss = 4.990082865572778\n",
      "tensor([[0.6291, 0.6739],\n",
      "        [0.5246, 0.5893],\n",
      "        [0.5526, 0.6610],\n",
      "        [0.6704, 0.7311],\n",
      "        [0.6129, 0.6205]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 11: Loss = 4.989977721955369\n",
      "tensor([[0.6291, 0.6739],\n",
      "        [0.5246, 0.5893],\n",
      "        [0.5526, 0.6610],\n",
      "        [0.6704, 0.7311],\n",
      "        [0.6129, 0.6205]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 12: Loss = 4.989872580898113\n",
      "tensor([[0.6291, 0.6739],\n",
      "        [0.5246, 0.5893],\n",
      "        [0.5526, 0.6610],\n",
      "        [0.6704, 0.7311],\n",
      "        [0.6129, 0.6205]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 13: Loss = 4.989767442400961\n",
      "tensor([[0.6291, 0.6739],\n",
      "        [0.5246, 0.5893],\n",
      "        [0.5526, 0.6611],\n",
      "        [0.6704, 0.7311],\n",
      "        [0.6129, 0.6205]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 14: Loss = 4.9896623064638606\n",
      "tensor([[0.6291, 0.6740],\n",
      "        [0.5246, 0.5893],\n",
      "        [0.5526, 0.6611],\n",
      "        [0.6704, 0.7311],\n",
      "        [0.6129, 0.6205]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 15: Loss = 4.989557173086761\n",
      "tensor([[0.6291, 0.6740],\n",
      "        [0.5246, 0.5892],\n",
      "        [0.5526, 0.6611],\n",
      "        [0.6704, 0.7311],\n",
      "        [0.6129, 0.6205]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 16: Loss = 4.989452042269614\n",
      "tensor([[0.6291, 0.6740],\n",
      "        [0.5246, 0.5892],\n",
      "        [0.5527, 0.6611],\n",
      "        [0.6704, 0.7311],\n",
      "        [0.6129, 0.6205]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 17: Loss = 4.989346914012364\n",
      "tensor([[0.6291, 0.6740],\n",
      "        [0.5246, 0.5892],\n",
      "        [0.5527, 0.6611],\n",
      "        [0.6704, 0.7311],\n",
      "        [0.6130, 0.6205]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 18: Loss = 4.989241788314963\n",
      "tensor([[0.6291, 0.6740],\n",
      "        [0.5246, 0.5892],\n",
      "        [0.5527, 0.6611],\n",
      "        [0.6704, 0.7311],\n",
      "        [0.6130, 0.6205]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 19: Loss = 4.989136665177361\n",
      "tensor([[0.6291, 0.6740],\n",
      "        [0.5246, 0.5892],\n",
      "        [0.5527, 0.6611],\n",
      "        [0.6704, 0.7311],\n",
      "        [0.6130, 0.6206]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 20: Loss = 4.989031544599505\n",
      "tensor([[0.6291, 0.6740],\n",
      "        [0.5246, 0.5892],\n",
      "        [0.5527, 0.6611],\n",
      "        [0.6704, 0.7312],\n",
      "        [0.6130, 0.6206]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 21: Loss = 4.9889264265813456\n",
      "tensor([[0.6291, 0.6740],\n",
      "        [0.5246, 0.5892],\n",
      "        [0.5527, 0.6611],\n",
      "        [0.6704, 0.7312],\n",
      "        [0.6130, 0.6206]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 22: Loss = 4.988821311122831\n",
      "tensor([[0.6291, 0.6740],\n",
      "        [0.5246, 0.5891],\n",
      "        [0.5527, 0.6611],\n",
      "        [0.6705, 0.7312],\n",
      "        [0.6130, 0.6206]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 23: Loss = 4.988716198223911\n",
      "tensor([[0.6291, 0.6740],\n",
      "        [0.5246, 0.5891],\n",
      "        [0.5527, 0.6611],\n",
      "        [0.6705, 0.7312],\n",
      "        [0.6130, 0.6206]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 24: Loss = 4.988611087884536\n",
      "tensor([[0.6291, 0.6740],\n",
      "        [0.5246, 0.5891],\n",
      "        [0.5527, 0.6611],\n",
      "        [0.6705, 0.7312],\n",
      "        [0.6130, 0.6206]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 25: Loss = 4.988505980104653\n",
      "tensor([[0.6291, 0.6740],\n",
      "        [0.5246, 0.5891],\n",
      "        [0.5527, 0.6611],\n",
      "        [0.6705, 0.7312],\n",
      "        [0.6130, 0.6206]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 26: Loss = 4.988400874884213\n",
      "tensor([[0.6291, 0.6740],\n",
      "        [0.5246, 0.5891],\n",
      "        [0.5527, 0.6612],\n",
      "        [0.6705, 0.7312],\n",
      "        [0.6130, 0.6206]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 27: Loss = 4.9882957722231644\n",
      "tensor([[0.6291, 0.6740],\n",
      "        [0.5246, 0.5891],\n",
      "        [0.5527, 0.6612],\n",
      "        [0.6705, 0.7312],\n",
      "        [0.6130, 0.6206]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 28: Loss = 4.9881906721214575\n",
      "tensor([[0.6292, 0.6741],\n",
      "        [0.5246, 0.5891],\n",
      "        [0.5527, 0.6612],\n",
      "        [0.6705, 0.7312],\n",
      "        [0.6130, 0.6206]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 29: Loss = 4.988085574579041\n",
      "tensor([[0.6292, 0.6741],\n",
      "        [0.5246, 0.5890],\n",
      "        [0.5528, 0.6612],\n",
      "        [0.6705, 0.7312],\n",
      "        [0.6130, 0.6206]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 30: Loss = 4.987980479595864\n",
      "tensor([[0.6292, 0.6741],\n",
      "        [0.5246, 0.5890],\n",
      "        [0.5528, 0.6612],\n",
      "        [0.6705, 0.7312],\n",
      "        [0.6130, 0.6207]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 31: Loss = 4.987875387171877\n",
      "tensor([[0.6292, 0.6741],\n",
      "        [0.5246, 0.5890],\n",
      "        [0.5528, 0.6612],\n",
      "        [0.6705, 0.7312],\n",
      "        [0.6130, 0.6207]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 32: Loss = 4.987770297307028\n",
      "tensor([[0.6292, 0.6741],\n",
      "        [0.5246, 0.5890],\n",
      "        [0.5528, 0.6612],\n",
      "        [0.6705, 0.7312],\n",
      "        [0.6130, 0.6207]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 33: Loss = 4.987665210001268\n",
      "tensor([[0.6292, 0.6741],\n",
      "        [0.5246, 0.5890],\n",
      "        [0.5528, 0.6612],\n",
      "        [0.6705, 0.7312],\n",
      "        [0.6131, 0.6207]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 34: Loss = 4.987560125254545\n",
      "tensor([[0.6292, 0.6741],\n",
      "        [0.5246, 0.5890],\n",
      "        [0.5528, 0.6612],\n",
      "        [0.6705, 0.7312],\n",
      "        [0.6131, 0.6207]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 35: Loss = 4.9874550430668085\n",
      "tensor([[0.6292, 0.6741],\n",
      "        [0.5246, 0.5890],\n",
      "        [0.5528, 0.6612],\n",
      "        [0.6705, 0.7312],\n",
      "        [0.6131, 0.6207]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 36: Loss = 4.987349963438009\n",
      "tensor([[0.6292, 0.6741],\n",
      "        [0.5246, 0.5889],\n",
      "        [0.5528, 0.6612],\n",
      "        [0.6705, 0.7312],\n",
      "        [0.6131, 0.6207]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 37: Loss = 4.987244886368096\n",
      "tensor([[0.6292, 0.6741],\n",
      "        [0.5246, 0.5889],\n",
      "        [0.5528, 0.6612],\n",
      "        [0.6705, 0.7312],\n",
      "        [0.6131, 0.6207]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 38: Loss = 4.987139811857019\n",
      "tensor([[0.6292, 0.6741],\n",
      "        [0.5246, 0.5889],\n",
      "        [0.5528, 0.6612],\n",
      "        [0.6705, 0.7312],\n",
      "        [0.6131, 0.6207]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 39: Loss = 4.987034739904727\n",
      "tensor([[0.6292, 0.6741],\n",
      "        [0.5246, 0.5889],\n",
      "        [0.5528, 0.6613],\n",
      "        [0.6705, 0.7313],\n",
      "        [0.6131, 0.6207]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 40: Loss = 4.986929670511169\n",
      "tensor([[0.6292, 0.6741],\n",
      "        [0.5246, 0.5889],\n",
      "        [0.5528, 0.6613],\n",
      "        [0.6705, 0.7313],\n",
      "        [0.6131, 0.6207]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 41: Loss = 4.986824603676296\n",
      "tensor([[0.6292, 0.6741],\n",
      "        [0.5246, 0.5889],\n",
      "        [0.5528, 0.6613],\n",
      "        [0.6705, 0.7313],\n",
      "        [0.6131, 0.6207]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 42: Loss = 4.986719539400057\n",
      "tensor([[0.6292, 0.6742],\n",
      "        [0.5246, 0.5889],\n",
      "        [0.5529, 0.6613],\n",
      "        [0.6705, 0.7313],\n",
      "        [0.6131, 0.6208]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 43: Loss = 4.9866144776824015\n",
      "tensor([[0.6292, 0.6742],\n",
      "        [0.5246, 0.5888],\n",
      "        [0.5529, 0.6613],\n",
      "        [0.6705, 0.7313],\n",
      "        [0.6131, 0.6208]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 44: Loss = 4.98650941852328\n",
      "tensor([[0.6292, 0.6742],\n",
      "        [0.5246, 0.5888],\n",
      "        [0.5529, 0.6613],\n",
      "        [0.6706, 0.7313],\n",
      "        [0.6131, 0.6208]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 45: Loss = 4.986404361922641\n",
      "tensor([[0.6292, 0.6742],\n",
      "        [0.5246, 0.5888],\n",
      "        [0.5529, 0.6613],\n",
      "        [0.6706, 0.7313],\n",
      "        [0.6131, 0.6208]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 46: Loss = 4.986299307880435\n",
      "tensor([[0.6293, 0.6742],\n",
      "        [0.5246, 0.5888],\n",
      "        [0.5529, 0.6613],\n",
      "        [0.6706, 0.7313],\n",
      "        [0.6131, 0.6208]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 47: Loss = 4.986194256396611\n",
      "tensor([[0.6293, 0.6742],\n",
      "        [0.5246, 0.5888],\n",
      "        [0.5529, 0.6613],\n",
      "        [0.6706, 0.7313],\n",
      "        [0.6131, 0.6208]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 48: Loss = 4.986089207471119\n",
      "tensor([[0.6293, 0.6742],\n",
      "        [0.5246, 0.5888],\n",
      "        [0.5529, 0.6613],\n",
      "        [0.6706, 0.7313],\n",
      "        [0.6131, 0.6208]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 49: Loss = 4.985984161103909\n",
      "tensor([[0.6293, 0.6742],\n",
      "        [0.5246, 0.5888],\n",
      "        [0.5529, 0.6613],\n",
      "        [0.6706, 0.7313],\n",
      "        [0.6131, 0.6208]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 50: Loss = 4.985879117294931\n",
      "tensor([[0.6293, 0.6742],\n",
      "        [0.5246, 0.5887],\n",
      "        [0.5529, 0.6613],\n",
      "        [0.6706, 0.7313],\n",
      "        [0.6132, 0.6208]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 51: Loss = 4.985774076044135\n",
      "tensor([[0.6293, 0.6742],\n",
      "        [0.5246, 0.5887],\n",
      "        [0.5529, 0.6613],\n",
      "        [0.6706, 0.7313],\n",
      "        [0.6132, 0.6208]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 52: Loss = 4.985669037351469\n",
      "tensor([[0.6293, 0.6742],\n",
      "        [0.5246, 0.5887],\n",
      "        [0.5529, 0.6613],\n",
      "        [0.6706, 0.7313],\n",
      "        [0.6132, 0.6208]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 53: Loss = 4.985564001216885\n",
      "tensor([[0.6293, 0.6742],\n",
      "        [0.5246, 0.5887],\n",
      "        [0.5529, 0.6614],\n",
      "        [0.6706, 0.7313],\n",
      "        [0.6132, 0.6209]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 54: Loss = 4.985458967640332\n",
      "tensor([[0.6293, 0.6742],\n",
      "        [0.5246, 0.5887],\n",
      "        [0.5530, 0.6614],\n",
      "        [0.6706, 0.7313],\n",
      "        [0.6132, 0.6209]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 55: Loss = 4.98535393662176\n",
      "tensor([[0.6293, 0.6742],\n",
      "        [0.5246, 0.5887],\n",
      "        [0.5530, 0.6614],\n",
      "        [0.6706, 0.7313],\n",
      "        [0.6132, 0.6209]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 56: Loss = 4.985248908161117\n",
      "tensor([[0.6293, 0.6743],\n",
      "        [0.5246, 0.5887],\n",
      "        [0.5530, 0.6614],\n",
      "        [0.6706, 0.7313],\n",
      "        [0.6132, 0.6209]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 57: Loss = 4.985143882258357\n",
      "tensor([[0.6293, 0.6743],\n",
      "        [0.5246, 0.5886],\n",
      "        [0.5530, 0.6614],\n",
      "        [0.6706, 0.7313],\n",
      "        [0.6132, 0.6209]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 58: Loss = 4.985038858913427\n",
      "tensor([[0.6293, 0.6743],\n",
      "        [0.5246, 0.5886],\n",
      "        [0.5530, 0.6614],\n",
      "        [0.6706, 0.7314],\n",
      "        [0.6132, 0.6209]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 59: Loss = 4.984933838126277\n",
      "tensor([[0.6293, 0.6743],\n",
      "        [0.5246, 0.5886],\n",
      "        [0.5530, 0.6614],\n",
      "        [0.6706, 0.7314],\n",
      "        [0.6132, 0.6209]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 60: Loss = 4.984828819896857\n",
      "tensor([[0.6293, 0.6743],\n",
      "        [0.5246, 0.5886],\n",
      "        [0.5530, 0.6614],\n",
      "        [0.6706, 0.7314],\n",
      "        [0.6132, 0.6209]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 61: Loss = 4.984723804225118\n",
      "tensor([[0.6293, 0.6743],\n",
      "        [0.5246, 0.5886],\n",
      "        [0.5530, 0.6614],\n",
      "        [0.6706, 0.7314],\n",
      "        [0.6132, 0.6209]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 62: Loss = 4.98461879111101\n",
      "tensor([[0.6293, 0.6743],\n",
      "        [0.5246, 0.5886],\n",
      "        [0.5530, 0.6614],\n",
      "        [0.6706, 0.7314],\n",
      "        [0.6132, 0.6209]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 63: Loss = 4.984513780554482\n",
      "tensor([[0.6294, 0.6743],\n",
      "        [0.5246, 0.5886],\n",
      "        [0.5530, 0.6614],\n",
      "        [0.6706, 0.7314],\n",
      "        [0.6132, 0.6209]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 64: Loss = 4.9844087725554855\n",
      "tensor([[0.6294, 0.6743],\n",
      "        [0.5246, 0.5885],\n",
      "        [0.5530, 0.6614],\n",
      "        [0.6706, 0.7314],\n",
      "        [0.6132, 0.6210]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 65: Loss = 4.9843037671139685\n",
      "tensor([[0.6294, 0.6743],\n",
      "        [0.5246, 0.5885],\n",
      "        [0.5530, 0.6614],\n",
      "        [0.6706, 0.7314],\n",
      "        [0.6132, 0.6210]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 66: Loss = 4.984198764229882\n",
      "tensor([[0.6294, 0.6743],\n",
      "        [0.5246, 0.5885],\n",
      "        [0.5530, 0.6615],\n",
      "        [0.6706, 0.7314],\n",
      "        [0.6133, 0.6210]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 67: Loss = 4.984093763903178\n",
      "tensor([[0.6294, 0.6743],\n",
      "        [0.5246, 0.5885],\n",
      "        [0.5531, 0.6615],\n",
      "        [0.6707, 0.7314],\n",
      "        [0.6133, 0.6210]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 68: Loss = 4.9839887661338045\n",
      "tensor([[0.6294, 0.6743],\n",
      "        [0.5246, 0.5885],\n",
      "        [0.5531, 0.6615],\n",
      "        [0.6707, 0.7314],\n",
      "        [0.6133, 0.6210]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 69: Loss = 4.983883770921712\n",
      "tensor([[0.6294, 0.6743],\n",
      "        [0.5246, 0.5885],\n",
      "        [0.5531, 0.6615],\n",
      "        [0.6707, 0.7314],\n",
      "        [0.6133, 0.6210]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 70: Loss = 4.98377877826685\n",
      "tensor([[0.6294, 0.6744],\n",
      "        [0.5246, 0.5885],\n",
      "        [0.5531, 0.6615],\n",
      "        [0.6707, 0.7314],\n",
      "        [0.6133, 0.6210]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 71: Loss = 4.98367378816917\n",
      "tensor([[0.6294, 0.6744],\n",
      "        [0.5246, 0.5884],\n",
      "        [0.5531, 0.6615],\n",
      "        [0.6707, 0.7314],\n",
      "        [0.6133, 0.6210]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 72: Loss = 4.983568800628622\n",
      "tensor([[0.6294, 0.6744],\n",
      "        [0.5246, 0.5884],\n",
      "        [0.5531, 0.6615],\n",
      "        [0.6707, 0.7314],\n",
      "        [0.6133, 0.6210]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 73: Loss = 4.983463815645155\n",
      "tensor([[0.6294, 0.6744],\n",
      "        [0.5246, 0.5884],\n",
      "        [0.5531, 0.6615],\n",
      "        [0.6707, 0.7314],\n",
      "        [0.6133, 0.6210]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 74: Loss = 4.983358833218721\n",
      "tensor([[0.6294, 0.6744],\n",
      "        [0.5246, 0.5884],\n",
      "        [0.5531, 0.6615],\n",
      "        [0.6707, 0.7314],\n",
      "        [0.6133, 0.6210]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 75: Loss = 4.983253853349269\n",
      "tensor([[0.6294, 0.6744],\n",
      "        [0.5246, 0.5884],\n",
      "        [0.5531, 0.6615],\n",
      "        [0.6707, 0.7314],\n",
      "        [0.6133, 0.6211]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 76: Loss = 4.98314887603675\n",
      "tensor([[0.6294, 0.6744],\n",
      "        [0.5246, 0.5884],\n",
      "        [0.5531, 0.6615],\n",
      "        [0.6707, 0.7314],\n",
      "        [0.6133, 0.6211]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 77: Loss = 4.983043901281114\n",
      "tensor([[0.6294, 0.6744],\n",
      "        [0.5246, 0.5884],\n",
      "        [0.5531, 0.6615],\n",
      "        [0.6707, 0.7315],\n",
      "        [0.6133, 0.6211]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 78: Loss = 4.982938929082311\n",
      "tensor([[0.6294, 0.6744],\n",
      "        [0.5246, 0.5883],\n",
      "        [0.5531, 0.6615],\n",
      "        [0.6707, 0.7315],\n",
      "        [0.6133, 0.6211]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 79: Loss = 4.982833959440292\n",
      "tensor([[0.6294, 0.6744],\n",
      "        [0.5246, 0.5883],\n",
      "        [0.5531, 0.6616],\n",
      "        [0.6707, 0.7315],\n",
      "        [0.6133, 0.6211]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 80: Loss = 4.982728992355007\n",
      "tensor([[0.6294, 0.6744],\n",
      "        [0.5246, 0.5883],\n",
      "        [0.5532, 0.6616],\n",
      "        [0.6707, 0.7315],\n",
      "        [0.6133, 0.6211]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 81: Loss = 4.982624027826406\n",
      "tensor([[0.6295, 0.6744],\n",
      "        [0.5246, 0.5883],\n",
      "        [0.5532, 0.6616],\n",
      "        [0.6707, 0.7315],\n",
      "        [0.6133, 0.6211]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 82: Loss = 4.98251906585444\n",
      "tensor([[0.6295, 0.6744],\n",
      "        [0.5246, 0.5883],\n",
      "        [0.5532, 0.6616],\n",
      "        [0.6707, 0.7315],\n",
      "        [0.6134, 0.6211]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 83: Loss = 4.982414106439059\n",
      "tensor([[0.6295, 0.6744],\n",
      "        [0.5246, 0.5883],\n",
      "        [0.5532, 0.6616],\n",
      "        [0.6707, 0.7315],\n",
      "        [0.6134, 0.6211]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 84: Loss = 4.982309149580214\n",
      "tensor([[0.6295, 0.6745],\n",
      "        [0.5246, 0.5883],\n",
      "        [0.5532, 0.6616],\n",
      "        [0.6707, 0.7315],\n",
      "        [0.6134, 0.6211]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 85: Loss = 4.9822041952778555\n",
      "tensor([[0.6295, 0.6745],\n",
      "        [0.5246, 0.5882],\n",
      "        [0.5532, 0.6616],\n",
      "        [0.6707, 0.7315],\n",
      "        [0.6134, 0.6211]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 86: Loss = 4.982099243531933\n",
      "tensor([[0.6295, 0.6745],\n",
      "        [0.5246, 0.5882],\n",
      "        [0.5532, 0.6616],\n",
      "        [0.6707, 0.7315],\n",
      "        [0.6134, 0.6211]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 87: Loss = 4.981994294342398\n",
      "tensor([[0.6295, 0.6745],\n",
      "        [0.5246, 0.5882],\n",
      "        [0.5532, 0.6616],\n",
      "        [0.6707, 0.7315],\n",
      "        [0.6134, 0.6212]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 88: Loss = 4.9818893477092\n",
      "tensor([[0.6295, 0.6745],\n",
      "        [0.5246, 0.5882],\n",
      "        [0.5532, 0.6616],\n",
      "        [0.6707, 0.7315],\n",
      "        [0.6134, 0.6212]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 89: Loss = 4.9817844036322905\n",
      "tensor([[0.6295, 0.6745],\n",
      "        [0.5246, 0.5882],\n",
      "        [0.5532, 0.6616],\n",
      "        [0.6708, 0.7315],\n",
      "        [0.6134, 0.6212]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 90: Loss = 4.98167946211162\n",
      "tensor([[0.6295, 0.6745],\n",
      "        [0.5246, 0.5882],\n",
      "        [0.5532, 0.6616],\n",
      "        [0.6708, 0.7315],\n",
      "        [0.6134, 0.6212]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 91: Loss = 4.981574523147138\n",
      "tensor([[0.6295, 0.6745],\n",
      "        [0.5246, 0.5882],\n",
      "        [0.5532, 0.6616],\n",
      "        [0.6708, 0.7315],\n",
      "        [0.6134, 0.6212]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 92: Loss = 4.981469586738797\n",
      "tensor([[0.6295, 0.6745],\n",
      "        [0.5246, 0.5881],\n",
      "        [0.5533, 0.6617],\n",
      "        [0.6708, 0.7315],\n",
      "        [0.6134, 0.6212]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 93: Loss = 4.981364652886546\n",
      "tensor([[0.6295, 0.6745],\n",
      "        [0.5246, 0.5881],\n",
      "        [0.5533, 0.6617],\n",
      "        [0.6708, 0.7315],\n",
      "        [0.6134, 0.6212]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 94: Loss = 4.981259721590336\n",
      "tensor([[0.6295, 0.6745],\n",
      "        [0.5246, 0.5881],\n",
      "        [0.5533, 0.6617],\n",
      "        [0.6708, 0.7315],\n",
      "        [0.6134, 0.6212]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 95: Loss = 4.981154792850117\n",
      "tensor([[0.6295, 0.6745],\n",
      "        [0.5246, 0.5881],\n",
      "        [0.5533, 0.6617],\n",
      "        [0.6708, 0.7315],\n",
      "        [0.6134, 0.6212]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 96: Loss = 4.981049866665842\n",
      "tensor([[0.6295, 0.6745],\n",
      "        [0.5246, 0.5881],\n",
      "        [0.5533, 0.6617],\n",
      "        [0.6708, 0.7316],\n",
      "        [0.6134, 0.6212]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 97: Loss = 4.980944943037459\n",
      "tensor([[0.6295, 0.6745],\n",
      "        [0.5246, 0.5881],\n",
      "        [0.5533, 0.6617],\n",
      "        [0.6708, 0.7316],\n",
      "        [0.6134, 0.6212]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 98: Loss = 4.98084002196492\n",
      "tensor([[0.6295, 0.6746],\n",
      "        [0.5246, 0.5881],\n",
      "        [0.5533, 0.6617],\n",
      "        [0.6708, 0.7316],\n",
      "        [0.6134, 0.6213]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r",
      "Epoch 99: Loss = 4.9807351034481755\n",
      "tensor([[0.6296, 0.6746],\n",
      "        [0.5246, 0.5880],\n",
      "        [0.5533, 0.6617],\n",
      "        [0.6708, 0.7316],\n",
      "        [0.6135, 0.6213]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\r"
     ]
    }
   ],
   "source": [
    "model = LabelModel()\n",
    "\n",
    "    \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    initial_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for data_sample, lf_outputs in data_loader:\n",
    "        \n",
    "\n",
    "        \n",
    "        marginal_prob = model(lf_outputs, true_label=1) + model(lf_outputs, true_label=-1)\n",
    "        log_marginal_prob = torch.log(marginal_prob)\n",
    "        initial_loss = initial_loss + log_marginal_prob\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        break\n",
    "    \n",
    "    initial_loss = -initial_loss\n",
    "    initial_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {initial_loss.item()}\")\n",
    "        for param in model.parameters():\n",
    "            print(torch.sigmoid(param),flush=True)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d433c686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
